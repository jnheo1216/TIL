{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
      "1.15.0\n",
      "loss : 0.9737863540649414\n",
      "loss : 0.694310188293457\n",
      "loss : 0.6932405829429626\n",
      "loss : 0.6931556463241577\n",
      "loss : 0.6931479573249817\n",
      "loss : 0.6931472420692444\n",
      "loss : 0.6931471824645996\n",
      "loss : 0.6931471824645996\n",
      "loss : 0.6931471824645996\n",
      "loss : 0.6931471824645996\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.50      0.40         2\n",
      "         1.0       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.25         4\n",
      "   macro avg       0.17      0.25      0.20         4\n",
      "weighted avg       0.17      0.25      0.20         4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "\n",
    "# TF 1.15버전으로 \n",
    "# GATE연산을 수행하는 Logistic Regression으로 구현해보아요!\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# Training Data Set\n",
    "x_data = np.array([[0,0],\n",
    "                   [0,1],\n",
    "                   [1,0],\n",
    "                   [1,1]], dtype=np.float32)\n",
    "\n",
    "t_data = np.array([[0],[1],[1],[0]], dtype=np.float32)\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([2,1]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(X,W) + b\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,\n",
    "                                                             labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-2).minimize(loss)\n",
    "\n",
    "\n",
    "# session, 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    _, loss_val = sess.run([train, loss], feed_dict={X:x_data,\n",
    "                                                     T:t_data})\n",
    "    if step % 3000 == 0:\n",
    "        print('loss : {}'.format(loss_val))\n",
    "\n",
    "# 성능평가(Accuracy)\n",
    "accuracy = tf.cast(H >= 0.5, dtype=tf.float32)\n",
    "result = sess.run(accuracy, feed_dict={X:x_data})\n",
    "print(classification_report(t_data.ravel(),result.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
      "1.15.0\n",
      "WARNING:tensorflow:From C:\\Users\\i\\anaconda3\\envs\\data_env\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "loss : 0.710208535194397\n",
      "loss : 0.4752802550792694\n",
      "loss : 0.21171966195106506\n",
      "loss : 0.10658705979585648\n",
      "loss : 0.06447659432888031\n",
      "loss : 0.04446244612336159\n",
      "loss : 0.03342806175351143\n",
      "loss : 0.026594560593366623\n",
      "loss : 0.021996958181262016\n",
      "loss : 0.018711965531110764\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00         2\n",
      "         1.0       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         4\n",
      "   macro avg       1.00      1.00      1.00         4\n",
      "weighted avg       1.00      1.00      1.00         4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "\n",
    "# TF 1.15버전으로 \n",
    "# GATE연산을 수행하는 Deep Learning으로 구현해보아요!\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# Training Data Set\n",
    "x_data = np.array([[0,0],\n",
    "                   [0,1],\n",
    "                   [1,0],\n",
    "                   [1,1]], dtype=np.float32)\n",
    "\n",
    "t_data = np.array([[0],[1],[1],[0]], dtype=np.float32)\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W2 = tf.Variable(tf.random.normal([2,100]), name='weight2')\n",
    "b2 = tf.Variable(tf.random.normal([100]), name='bias2')\n",
    "layer2 = tf.sigmoid(tf.matmul(X,W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random.normal([100,6]), name='weight3')\n",
    "b3 = tf.Variable(tf.random.normal([6]), name='bias3')\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2,W3) + b3)\n",
    "\n",
    "W4 = tf.Variable(tf.random.normal([6,1]), name='weight4')\n",
    "b4 = tf.Variable(tf.random.normal([1]), name='bias4')\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(layer3,W4) + b4\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,\n",
    "                                                             labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-2).minimize(loss)\n",
    "\n",
    "\n",
    "# session, 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    _, loss_val = sess.run([train, loss], feed_dict={X:x_data,\n",
    "                                                     T:t_data})\n",
    "    if step % 3000 == 0:\n",
    "        print('loss : {}'.format(loss_val))\n",
    "\n",
    "# 성능평가(Accuracy)\n",
    "accuracy = tf.cast(H >= 0.5, dtype=tf.float32)\n",
    "result = sess.run(accuracy, feed_dict={X:x_data})\n",
    "print(classification_report(t_data.ravel(),result.ravel()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_env] *",
   "language": "python",
   "name": "conda-env-data_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
